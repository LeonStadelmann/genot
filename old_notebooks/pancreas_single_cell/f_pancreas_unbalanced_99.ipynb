{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/dominik.klein/mambaforge/envs/entot_pip/lib/python3.11/site-packages/equinox/_ad.py:753: UserWarning: As of Equinox 0.10.7, `equinox.filter_custom_vjp.defvjp` is deprecated in favour of `.def_fwd` and `.def_bwd`. This new API supports symbolic zeros, which allow for more efficient autodifferentiation rules. In particular:\n",
      "- the fwd and bwd functions take an extra `perturbed` argument, which     indicates which primals actually need a gradient. You can use this     to skip computing the gradient for any unperturbed value. (You can     also safely just ignore this if you wish.)\n",
      "- `None` was previously passed to indicate a symbolic zero gradient for     all objects that weren't inexact arrays, but all inexact arrays     always had an array-valued gradient. Now, `None` may also be passed     to indicate that an inexact array has a symbolic zero gradient.\n",
      "  warnings.warn(\n",
      "2023-09-18 11:08:20.552720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/icb/dominik.klein/mambaforge/envs/entot_pip/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import scanpy as sc\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as pp\n",
    "import numpy as np\n",
    "\n",
    "import ott\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from ott.geometry import geometry, pointcloud\n",
    "import jax\n",
    "from typing import Mapping, Any, Optional, Union, Callable, Tuple\n",
    "from types import MappingProxyType\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "from ott.solvers.linear import sinkhorn\n",
    "from ott.problems.linear import linear_problem\n",
    "from entot.models.model import OTFlowMatching\n",
    "from entot.nets.nets import MLP_vector_field, MLP_bridge, MLP_marginal,MLP_fused_vector_field\n",
    "import sklearn.preprocessing as pp\n",
    "import scanpy as sc\n",
    "from ott.solvers.linear import sinkhorn, acceleration\n",
    "from sklearn import preprocessing as pp\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.tools.sinkhorn_divergence import sinkhorn_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"../../data/adata_pancreas_2019.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs[\"celltype\"]!=\"Multipotent\"].copy()\n",
    "adata.obs[\"lineage\"] = adata.obs.apply(lambda x: \"A\" if x[\"celltype\"] in [\"Acinar\", \"Tip\"] else \"ED\", axis=1).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.pca(adata, n_comps=30)\n",
    "adata.obsm[\"X_pca_scaled\"] = pp.StandardScaler().fit_transform(adata.obsm[\"X_pca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable, Union, List, Optional\n",
    "import scipy.sparse as sp\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import pandas as pd\n",
    "\n",
    "def get_nearest_neighbors(\n",
    "    input_batch: jnp.ndarray, target: jnp.ndarray, k: int = 30\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Get the k nearest neighbors of the input batch in the target.\"\"\"\n",
    "    if target.shape[0] < k:\n",
    "        raise ValueError(f\"k is {k}, but must be smaller or equal than {target.shape[0]}.\")\n",
    "    pairwise_euclidean_distances = jnp.sqrt(jnp.sum((input_batch - target) ** 2, axis=-1))\n",
    "    negative_distances, indices = jax.lax.top_k(-1 * pairwise_euclidean_distances, k=k)\n",
    "    return -1 * negative_distances, indices\n",
    "\n",
    "def project_transport_matrix(  \n",
    "        predicted_tgt_cells: jnp.ndarray,\n",
    "        tgt_cells: jnp.ndarray,\n",
    "        batch_size: int = 1024,\n",
    "        k: int = 1,\n",
    "    ) -> sp.csr_matrix:\n",
    "        \"\"\"Project Neural OT map onto cells.\"\"\"\n",
    "\n",
    "        get_knn_fn = jax.vmap(get_nearest_neighbors, in_axes=(0, None, None))\n",
    "        row_indices: Union[jnp.ndarray, List[jnp.ndarray]] = []\n",
    "        column_indices: Union[jnp.ndarray, List[jnp.ndarray]] = []\n",
    "        distances_list: Union[jnp.ndarray, List[jnp.ndarray]] = []\n",
    "        for index in range(0, len(predicted_tgt_cells), batch_size):\n",
    "            _, indices = get_knn_fn(predicted_tgt_cells[index : index + batch_size], tgt_cells, k)\n",
    "            column_indices.append(indices.flatten())\n",
    "            row_indices.append(\n",
    "                jnp.repeat(jnp.arange(index, index + min(batch_size, len(predicted_tgt_cells) - index)), min(k, len(tgt_cells)))\n",
    "            )\n",
    "        ri = jnp.concatenate(row_indices)\n",
    "        ci = jnp.concatenate(column_indices)\n",
    "        mat = np.zeros((len(ri), len(tgt_cells)))\n",
    "        mat[ri,ci] = 1.0\n",
    "        return mat\n",
    "\n",
    "def aggregate_transport_matrix(adata_source, adata_target, tmat, aggregation_key = \"celltype\", forward = True):\n",
    "    df_source = adata_source.obs[aggregation_key]\n",
    "    df_target = adata_target.obs[aggregation_key]\n",
    "\n",
    "    annotations_source = adata_source.obs[aggregation_key].cat.categories\n",
    "    annotations_target = adata_target.obs[aggregation_key].cat.categories\n",
    "\n",
    "    tm = pd.DataFrame(\n",
    "        np.zeros((len(annotations_source), len(annotations_target))),\n",
    "        index=annotations_source,\n",
    "        columns=annotations_target,\n",
    "    )\n",
    "    \n",
    "    for annotation_src in annotations_source:\n",
    "        for annotation_tgt in annotations_target:\n",
    "            tm.loc[annotation_src, annotation_tgt] = tmat[\n",
    "                np.ix_((df_source == annotation_src).squeeze(), (df_target == annotation_tgt).squeeze())\n",
    "            ].sum()\n",
    "    return tm#.div(tm.sum(axis=int(forward)), axis=int(not forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.pca(adata, n_comps=30)\n",
    "adata.obsm[\"X_pca_scaled\"] = pp.StandardScaler().fit_transform(adata.obsm[\"X_pca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:09:05.593117: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.128). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "n_cells_source=len(adata[adata.obs[\"day\"]==\"14.5\"])\n",
    "n_cells_target=len(adata[adata.obs[\"day\"]==\"15.5\"])\n",
    "\n",
    "n_samples_train_source = int(n_cells_source * 0.6)\n",
    "n_samples_test_source = n_cells_source - n_samples_train_source\n",
    "\n",
    "n_samples_train_target = int(n_cells_target * 0.6)\n",
    "n_samples_test_target = n_cells_target - n_samples_train_target\n",
    "\n",
    "inds_source_train = np.asarray(jax.random.choice(jax.random.PRNGKey(0), n_cells_source, (n_samples_train_source,), replace=False))\n",
    "inds_source_test = list(set(list(range(n_samples_train_source))) - set(inds_source_train))\n",
    "\n",
    "inds_target_train = np.asarray(list(jax.random.choice(jax.random.PRNGKey(1), n_cells_target, (n_samples_train_target,), replace=False)))\n",
    "inds_target_test = list(set(list(range(n_samples_train_target))) - set(inds_target_train))\n",
    "\n",
    "adata_source_train = adata[adata.obs[\"day\"]==\"14.5\"][inds_source_train,:]\n",
    "adata_source_test = adata[adata.obs[\"day\"]==\"14.5\"][inds_source_test,:]\n",
    "\n",
    "adata_target_train = adata[adata.obs[\"day\"]==\"15.5\"][inds_target_train,:]\n",
    "adata_target_test = adata[adata.obs[\"day\"]==\"15.5\"][inds_target_test,:]\n",
    "\n",
    "source_train = adata_source_train.obsm[\"X_pca_scaled\"]\n",
    "source_test = adata_source_test.obsm[\"X_pca_scaled\"]\n",
    "target_train = adata_target_train.obsm[\"X_pca_scaled\"]\n",
    "target_test = adata_target_test.obsm[\"X_pca_scaled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test = sc.concat(\n",
    "    [adata_source_test, adata_target_test],\n",
    "    join=\"outer\",\n",
    "    label=\"day\",\n",
    "    keys=[\"14.5\", \"15.5\"],\n",
    ")\n",
    "\n",
    "adata_train = sc.concat(\n",
    "    [adata_source_train, adata_target_train],\n",
    "    join=\"outer\",\n",
    "    label=\"day\",\n",
    "    keys=[14.5, 15.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:32<00:00, 22.12it/s]\n",
      "100%|██████████| 10000/10000 [07:32<00:00, 22.12it/s]\n",
      "100%|██████████| 10000/10000 [07:31<00:00, 22.14it/s]\n"
     ]
    }
   ],
   "source": [
    "sinkhorn_divs = [None] * 3\n",
    "scores = [None] * 3\n",
    "\n",
    "for i in range(3):\n",
    "    ot_solver=ott.solvers.linear.sinkhorn.Sinkhorn()\n",
    "    neural_net = MLP_vector_field(target_train.shape[1], latent_embed_dim = 256, num_layers=8, n_frequencies=128)\n",
    "    bridge_net = MLP_bridge(target_train.shape[1], 10)\n",
    "    \n",
    "    \n",
    "    mlp_eta = MLP_marginal(256, 5)\n",
    "    mlp_xi = MLP_marginal(256, 5)\n",
    "    \n",
    "    \n",
    "    otfm = OTFlowMatching(neural_net, scale_cost=\"mean\", bridge_net=bridge_net, ot_solver=ot_solver, epsilon=1e-2, mlp_eta=mlp_eta, mlp_xi=mlp_xi, tau_a=0.99, tau_b=0.99, input_dim=30, output_dim=30, iterations=10_000, k_noise_per_x=1, seed=i)\n",
    "    otfm(source_train, target_train, 1024, 1024)\n",
    "    gex_predicted = otfm.transport(source_test, seed=0)[0][0,...]\n",
    "    sinkhorn_divs[i] = sinkhorn_divergence(PointCloud, gex_predicted, target_test, epsilon=1e-3).divergence\n",
    "    \n",
    "\n",
    "    tm = project_transport_matrix(gex_predicted, target_test)\n",
    "    agg_tm = aggregate_transport_matrix(adata_source_test, adata_target_test, tm, aggregation_key=\"lineage\")\n",
    "    scores[i] = (agg_tm.iloc[0,0]+agg_tm.iloc[1,1])/agg_tm.sum().sum()\n",
    "\n",
    "    full_test = np.concatenate((source_test, target_test), axis=0)\n",
    "    adata_test.obs[f\"left_rescaling_{i}\"] = otfm.state_eta.apply_fn({\"params\": otfm.state_eta.params}, x=full_test)\n",
    "    adata_test.obs[f\"right_rescaling_{i}\"] = otfm.state_eta.apply_fn({\"params\": otfm.state_xi.params}, x=full_test)\n",
    "\n",
    "    full_train = np.concatenate((source_train, target_train), axis=0)\n",
    "    adata_train.obs[f\"left_rescaling_{i}\"] = otfm.state_eta.apply_fn({\"params\": otfm.state_eta.params}, x=full_train)\n",
    "    adata_train.obs[f\"right_rescaling_{i}\"] = otfm.state_eta.apply_fn({\"params\": otfm.state_xi.params}, x=full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train.obs.to_csv(\"pancreas_unbalanced_99_genot_train.csv\")\n",
    "adata_test.obs.to_csv(\"pancreas_unbalanced_99_genot_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7137886037407567 0.0006542549940865413 19.631609 0.09984531\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores), np.var(scores), np.mean(sinkhorn_divs), np.var(sinkhorn_divs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entot_pip",
   "language": "python",
   "name": "entot_pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
